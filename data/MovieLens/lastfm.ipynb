{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. 安装并配置 Kaggle CLI（将 your kaggle.json 上传到 /content）\n",
        "%cd /content\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiNi5vmnYxKB",
        "outputId": "b30188cb-f271-4a83-8db0-5293126a02fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allbut.pl     sample_data  u2.test  u4.test  ua.test  u.genre\t    u.user\n",
            "lastfm1k.zip  u1.base\t   u3.base  u5.base  ub.base  u.info\n",
            "mku.sh\t      u1.test\t   u3.test  u5.test  ub.test  u.item\n",
            "README\t      u2.base\t   u4.base  ua.base  u.data   u.occupation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, random\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# ── 一、路径 & 参数 ───────────────────────────────────────────\n",
        "RAW_DIR    = '/content'               # 数据就放在 /content 下\n",
        "OUT_DIR    = '/content/ml100k_proc'\n",
        "LGN_DIR    = os.path.join(OUT_DIR, 'lgn')\n",
        "MIN_HIST   = 5\n",
        "MAX_HIST   = 30\n",
        "CANDIDATES = 20\n",
        "random.seed(42)\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(LGN_DIR, exist_ok=True)\n",
        "\n",
        "# ── 二、加载 interaction 日志 (u.data) ────────────────────────\n",
        "data_path = os.path.join(RAW_DIR, 'u.data')\n",
        "df = pd.read_csv(data_path, sep='\\t',\n",
        "                 names=['user_id','item_id','rating','timestamp'])\n",
        "df = df.sort_values('timestamp')\n",
        "print(\"Loaded interactions:\", df.shape)\n",
        "\n",
        "# ── 三、加载 item 元数据 (u.item) ─────────────────────────────\n",
        "item_path = os.path.join(RAW_DIR, 'u.item')\n",
        "items = {}\n",
        "with open(item_path, encoding='latin-1') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split('|')\n",
        "        mid = int(parts[0])\n",
        "        title = parts[1]\n",
        "        items[mid] = {'title': title}\n",
        "print(\"Loaded item_meta:\", len(items))\n",
        "\n",
        "# ── 四、构建 & 过滤用户序列 ─────────────────────────────────\n",
        "user_seqs = df.groupby('user_id')['item_id'].apply(list).to_dict()\n",
        "user_seqs = {\n",
        "    u: seq[-MAX_HIST:]\n",
        "    for u, seq in user_seqs.items()\n",
        "    if len(seq) >= MIN_HIST + 1\n",
        "}\n",
        "print(\"Active users:\", len(user_seqs))\n",
        "\n",
        "# ── 五、生成 (history→next) 样本 & 拆分 Train/Cal/Test ─────────────\n",
        "samples = []\n",
        "for u, seq in user_seqs.items():\n",
        "    for t in range(MIN_HIST, len(seq)):\n",
        "        samples.append((u, seq[:t], seq[t]))\n",
        "\n",
        "# 按用户聚集\n",
        "by_user = defaultdict(list)\n",
        "for u, h, n in samples:\n",
        "    by_user[u].append((h, n))\n",
        "\n",
        "train_samps, cal_samps, test_samps = [], [], []\n",
        "for u, recs in by_user.items():\n",
        "    *rest, last = recs\n",
        "    # Test：最后一次交互\n",
        "    test_samps.append((u, last[0], last[1]))\n",
        "    # Cal：倒数第二次\n",
        "    cal_samps.append((u, rest[-1][0], rest[-1][1]))\n",
        "    # Train：其余（注意这里要加 u）\n",
        "    for hist, nxt in rest[:-1]:\n",
        "        train_samps.append((u, hist, nxt))\n",
        "\n",
        "print(f\"Train={len(train_samps)}, Cal={len(cal_samps)}, Test={len(test_samps)}\")\n",
        "\n",
        "# ── 六、负采样函数 ────────────────────────────────────────────\n",
        "all_items = list(items.keys())\n",
        "def sample_cands(hist, nxt):\n",
        "    pool = list(set(all_items) - set(hist) - {nxt})\n",
        "    negs = (random.sample(pool, CANDIDATES-1)\n",
        "            if len(pool) >= CANDIDATES-1\n",
        "            else random.choices(pool, k=CANDIDATES-1))\n",
        "    cands = [nxt] + negs\n",
        "    random.shuffle(cands)\n",
        "    return cands\n",
        "\n",
        "# ── 七、写 JSONL ─────────────────────────────────────────────\n",
        "def write_jsonl(name, splits):\n",
        "    path = os.path.join(OUT_DIR, f\"{name}.jsonl\")\n",
        "    with open(path, 'w', encoding='utf-8') as out:\n",
        "        for u, h, n in splits:\n",
        "            cands = sample_cands(h, n)\n",
        "            rec = {\n",
        "                'user_id': u,\n",
        "                'history': h,\n",
        "                'history_titles': [items[i]['title'] for i in h],\n",
        "                'candidates': cands,\n",
        "                'candidates_titles': [items[i]['title'] for i in cands],\n",
        "                'label': n,\n",
        "                'label_title': items[n]['title']\n",
        "            }\n",
        "            out.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
        "    print(f\"Wrote {name}.jsonl ({len(splits)} samples)\")\n",
        "\n",
        "for split, data in [('train', train_samps),\n",
        "                    ('cal',   cal_samps),\n",
        "                    ('test',  test_samps)]:\n",
        "    write_jsonl(split, data)\n",
        "\n",
        "# ── 八、写 LightGCN 格式 & 统计分析 ──────────────────────────\n",
        "with open(os.path.join(LGN_DIR, 'train.txt'), 'w') as ft, \\\n",
        "     open(os.path.join(LGN_DIR, 'test.txt'),  'w') as fe:\n",
        "    for u, seq in user_seqs.items():\n",
        "        ft.write(f\"{u} \" + \" \".join(map(str, seq[:-1])) + \"\\n\")\n",
        "        fe.write(f\"{u} \" + str(seq[-1]) + \"\\n\")\n",
        "print(\"Wrote LightGCN files\")\n",
        "\n",
        "for split in ['train','cal','test']:\n",
        "    df_s = pd.read_json(os.path.join(OUT_DIR, f\"{split}.jsonl\"), lines=True)\n",
        "    print(f\"\\n=== {split.upper()} STATISTICS ===\")\n",
        "    print(\"Samples:\", len(df_s))\n",
        "    print(\"Users:\", df_s['user_id'].nunique())\n",
        "    items_all = set(df_s['history'].explode()) | set(df_s['candidates'].explode())\n",
        "    print(\"Items:\", len(items_all))\n",
        "    stats = df_s['history'].apply(len).describe()[['min','25%','50%','75%','max']]\n",
        "    print(\"History lengths:\", stats.to_dict())\n",
        "    print(\"Candidate list lengths:\", sorted(df_s['candidates'].apply(len).unique()))\n",
        "    pos = df_s.apply(lambda r: r['candidates'].index(r['label']), axis=1)\n",
        "    print(\"Label position distribution:\", pos.value_counts().sort_index().to_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVsKxMAAc_fQ",
        "outputId": "881d4c87-e705-4630-b8e8-fb3f75d94e6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded interactions: (100000, 4)\n",
            "Loaded item_meta: 1682\n",
            "Active users: 943\n",
            "Train=20450, Cal=943, Test=943\n",
            "Wrote train.jsonl (20450 samples)\n",
            "Wrote cal.jsonl (943 samples)\n",
            "Wrote test.jsonl (943 samples)\n",
            "Wrote LightGCN files\n",
            "\n",
            "=== TRAIN STATISTICS ===\n",
            "Samples: 20450\n",
            "Users: 943\n",
            "Items: 1682\n",
            "History lengths: {'min': 5.0, '25%': 10.0, '50%': 15.0, '75%': 21.0, 'max': 27.0}\n",
            "Candidate list lengths: [np.int64(20)]\n",
            "Label position distribution: {0: 950, 1: 1063, 2: 1037, 3: 992, 4: 1032, 5: 995, 6: 1011, 7: 1055, 8: 1027, 9: 1028, 10: 1083, 11: 1048, 12: 1007, 13: 1003, 14: 1048, 15: 1006, 16: 1002, 17: 1038, 18: 978, 19: 1047}\n",
            "\n",
            "=== CAL STATISTICS ===\n",
            "Samples: 943\n",
            "Users: 943\n",
            "Items: 1682\n",
            "History lengths: {'min': 18.0, '25%': 28.0, '50%': 28.0, '75%': 28.0, 'max': 28.0}\n",
            "Candidate list lengths: [np.int64(20)]\n",
            "Label position distribution: {0: 44, 1: 37, 2: 50, 3: 48, 4: 48, 5: 54, 6: 53, 7: 39, 8: 51, 9: 34, 10: 54, 11: 42, 12: 54, 13: 54, 14: 36, 15: 58, 16: 52, 17: 42, 18: 53, 19: 40}\n",
            "\n",
            "=== TEST STATISTICS ===\n",
            "Samples: 943\n",
            "Users: 943\n",
            "Items: 1682\n",
            "History lengths: {'min': 19.0, '25%': 29.0, '50%': 29.0, '75%': 29.0, 'max': 29.0}\n",
            "Candidate list lengths: [np.int64(20)]\n",
            "Label position distribution: {0: 55, 1: 40, 2: 44, 3: 41, 4: 49, 5: 54, 6: 55, 7: 34, 8: 53, 9: 39, 10: 46, 11: 51, 12: 52, 13: 43, 14: 46, 15: 36, 16: 51, 17: 53, 18: 52, 19: 49}\n"
          ]
        }
      ]
    }
  ]
}